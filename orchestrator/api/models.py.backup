"""
Pydantic models for API request/response schemas
Includes both custom endpoints and OpenAI-compatible chat completions
"""
from pydantic import BaseModel, Field, field_validator
from typing import Optional, List, Literal
import time
import uuid

class QueryRequest(BaseModel):
    """Request model for /query endpoint"""
    query: str = Field(..., description="User query to process")
    max_tokens: int = Field(1024, description="Maximum tokens in response")
    use_rag: Optional[bool] = Field(None, description="Force RAG on/off (None=auto)")
    top_k: int = Field(5, description="Number of context chunks to retrieve")

class QueryResponse(BaseModel):
    """Response model for /query endpoint"""
    response: str = Field(..., description="Generated response")
    model_used: str = Field(..., description="Which model generated the response")
    rag_used: bool = Field(..., description="Whether RAG was used")
    context_chunks: int = Field(0, description="Number of context chunks used")
    latency_ms: float = Field(..., description="Total latency in milliseconds")
    route_decision: str = Field(..., description="Router classification result")

class EmbeddingRequest(BaseModel):
    """Request model for /embed endpoint"""
    texts: List[str] = Field(..., description="List of texts to embed")

class EmbeddingResponse(BaseModel):
    """Response model for /embed endpoint"""
    embeddings: List[List[float]] = Field(..., description="List of embedding vectors")
    dimension: int = Field(..., description="Embedding dimension")
    count: int = Field(..., description="Number of embeddings")

class IndexRequest(BaseModel):
    """Request model for /index endpoint"""
    documents: List[str] = Field(..., description="Documents to index")
    metadata: Optional[List[dict]] = Field(None, description="Optional metadata per document")

class IndexResponse(BaseModel):
    """Response model for /index endpoint"""
    indexed_count: int = Field(..., description="Number of documents indexed")
    collection: str = Field(..., description="Qdrant collection name")
    latency_ms: float = Field(..., description="Indexing latency")


# ============================================================================
# OpenAI-Compatible Chat Completion Models
# ============================================================================

class ChatMessage(BaseModel):
    """Single message in a chat conversation"""
    role: Literal["system", "user", "assistant"] = Field(
        ...,
        description="Role of the message sender"
    )
    content: str = Field(..., description="Content of the message")
    name: Optional[str] = Field(None, description="Optional name of the sender")


class ChatCompletionRequest(BaseModel):
    """
    OpenAI-compatible chat completion request
    Supports drop-in replacement for OpenAI API clients
    """
    messages: List[ChatMessage] = Field(
        ...,
        description="List of messages in the conversation",
        min_length=1
    )
    model: str = Field(
        default="gpt-oss-120b",
        description="Model to use (auto-routed internally)"
    )
    temperature: float = Field(
        default=0.7,
        ge=0.0,
        le=2.0,
        description="Sampling temperature (0-2)"
    )
    max_tokens: int = Field(
        default=1024,
        gt=0,
        description="Maximum tokens to generate"
    )
    top_p: Optional[float] = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="Nucleus sampling parameter"
    )
    stream: bool = Field(
        default=False,
        description="Whether to stream responses (not yet supported)"
    )
    user: Optional[str] = Field(
        None,
        description="Unique user identifier for tracking and memory"
    )
    # Memory and thinking controls
    use_memory: bool = Field(
        default=True,
        description="Enable memory retrieval and storage"
    )
    use_sequential_thinking: bool = Field(
        default=True,
        description="Enable sequential thinking before processing"
    )
    top_k: int = Field(
        default=5,
        description="Number of memory/context chunks to retrieve"
    )


class ChatCompletionChoice(BaseModel):
    """Single choice in a chat completion response"""
    index: int = Field(..., description="Choice index")
    message: ChatMessage = Field(..., description="Generated message")
    finish_reason: Literal["stop", "length", "error"] = Field(
        ...,
        description="Reason for completion finish"
    )


class ChatCompletionUsage(BaseModel):
    """Token usage statistics"""
    prompt_tokens: int = Field(..., description="Tokens in prompt")
    completion_tokens: int = Field(..., description="Tokens in completion")
    total_tokens: int = Field(..., description="Total tokens used")


class ChatCompletionResponse(BaseModel):
    """
    OpenAI-compatible chat completion response
    Includes additional metadata about routing and memory
    """
    id: str = Field(
        default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex[:24]}",
        description="Unique completion ID"
    )
    object: str = Field(
        default="chat.completion",
        description="Object type"
    )
    created: int = Field(
        default_factory=lambda: int(time.time()),
        description="Unix timestamp of creation"
    )
    model: str = Field(..., description="Model used for generation")
    choices: List[ChatCompletionChoice] = Field(
        ...,
        description="List of completion choices"
    )
    usage: ChatCompletionUsage = Field(..., description="Token usage stats")

    # Additional metadata (custom extensions)
    route_decision: Optional[str] = Field(
        None,
        description="Router classification: simple/complex/factual"
    )
    rag_used: Optional[bool] = Field(
        None,
        description="Whether RAG was used"
    )
    memory_used: Optional[bool] = Field(
        None,
        description="Whether memory was retrieved"
    )
    thinking_used: Optional[bool] = Field(
        None,
        description="Whether sequential thinking was used"
    )
