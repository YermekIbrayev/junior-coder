/opt/vision_model/venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field "model_used" in QueryResponse has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
INFO:     Started server process [3608189]
INFO:     Waiting for application startup.
2025-11-14 04:01:14 - INFO - [main:22] - üöÄ Starting orchestrator initialization...
2025-11-14 04:01:14 - DEBUG - [main:26] - Initializing HTTP clients...
2025-11-14 04:01:14 - DEBUG - [services.llm_client:19] - Initializing HTTP client...
2025-11-14 04:01:14 - INFO - [services.llm_client:24] - ‚úì HTTP client initialized (connection pool: 100 max, 20 keepalive, timeout: 120s)
2025-11-14 04:01:14 - INFO - [main:28] - ‚úì HTTP clients initialized successfully
2025-11-14 04:01:14 - DEBUG - [main:31] - Initializing Mem0 client (Qdrant: http://localhost:6333, Collection: agent_memories)
2025-11-14 04:01:14 - INFO - [main:37] - ‚úì Mem0 memory client initialized (collection: agent_memories)
2025-11-14 04:01:14 - INFO - [main:38] - ‚úÖ Orchestrator ready to handle requests
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
INFO:     127.0.0.1:49092 - "POST /health HTTP/1.1" 405 Method Not Allowed
2025-11-14 04:07:30 - DEBUG - [main:79] - Health check requested
2025-11-14 04:07:30 - DEBUG - [main:89] - Health check response: {'status': 'healthy', 'services': {'gpt_oss': 'http://192.168.51.22:5000', 'bge_m3': 'http://192.168.51.22:8001', 'qwen_router': 'http://192.168.51.22:8002', 'qdrant': 'http://localhost:6333'}}
INFO:     127.0.0.1:54696 - "GET /health HTTP/1.1" 200 OK
2025-11-14 04:09:30 - ERROR - [api.routes:330] - [5688c094] ‚ùå Unexpected error: Invalid \escape: line 1 column 67 (char 66)
Traceback (most recent call last):
  File "/opt/vision_model/orchestrator/api/routes.py", line 151, in chat_completions_endpoint
    body = await request.json()
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/vision_model/venv/lib/python3.12/site-packages/starlette/requests.py", line 242, in json
    self._json = json.loads(body)
                 ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Invalid \escape: line 1 column 67 (char 66)
INFO:     127.0.0.1:33426 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
2025-11-14 09:02:43 - INFO - [api.routes:158] - [7d9a7e73] Chat completion request received: user=test-user, messages=1, model=gpt-4, temp=0.7, max_tokens=50
2025-11-14 09:02:43 - DEBUG - [api.routes:171] - [7d9a7e73] Extracted query (length=6): Hello!...
2025-11-14 09:02:43 - DEBUG - [api.routes:176] - [7d9a7e73] STEP 1: Retrieving memories for user=test-user
2025-11-14 09:02:43 - DEBUG - [httpcore.connection:87] - connect_tcp.started host='192.168.51.22' port=8001 local_address=None timeout=30.0 socket_options=None
2025-11-14 09:02:43 - DEBUG - [httpcore.connection:87] - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f09e80fabd0>
2025-11-14 09:02:43 - DEBUG - [httpcore.http11:87] - send_request_headers.started request=<Request [b'POST']>
2025-11-14 09:02:43 - DEBUG - [httpcore.http11:87] - send_request_headers.complete
2025-11-14 09:02:43 - DEBUG - [httpcore.http11:87] - send_request_body.started request=<Request [b'POST']>
2025-11-14 09:02:43 - DEBUG - [httpcore.http11:87] - send_request_body.complete
2025-11-14 09:02:43 - DEBUG - [httpcore.http11:87] - receive_response_headers.started request=<Request [b'POST']>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 14 Nov 2025 09:02:42 GMT'), (b'server', b'uvicorn'), (b'content-length', b'21915'), (b'content-type', b'application/json')])
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - receive_response_body.started request=<Request [b'POST']>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - receive_response_body.complete
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - response_closed.started
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - response_closed.complete
2025-11-14 09:02:44 - DEBUG - [httpcore.connection:47] - connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2025-11-14 09:02:44 - DEBUG - [httpcore.connection:47] - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f09e81a99d0>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - send_request_headers.started request=<Request [b'POST']>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - send_request_headers.complete
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - send_request_body.started request=<Request [b'POST']>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - send_request_body.complete
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - receive_response_headers.started request=<Request [b'POST']>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'gzip'), (b'content-type', b'application/json'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Fri, 14 Nov 2025 09:02:44 GMT')])
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - receive_response_body.started request=<Request [b'POST']>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - receive_response_body.complete
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - response_closed.started
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:47] - response_closed.complete
2025-11-14 09:02:44 - DEBUG - [httpcore.connection:47] - close.started
2025-11-14 09:02:44 - DEBUG - [httpcore.connection:47] - close.complete
2025-11-14 09:02:44 - INFO - [api.routes:189] - [7d9a7e73] ‚úì Retrieved 1 memories (total chars: 502)
2025-11-14 09:02:44 - DEBUG - [api.routes:198] - [7d9a7e73] STEP 2: Performing sequential thinking
2025-11-14 09:02:44 - DEBUG - [services.llm_client:63] - Calling LLM: url=http://192.168.51.22:8002/v1/chat/completions, messages=2, max_tokens=256, temp=0.3
2025-11-14 09:02:44 - DEBUG - [httpcore.connection:87] - connect_tcp.started host='192.168.51.22' port=8002 local_address=None timeout=120 socket_options=None
2025-11-14 09:02:44 - DEBUG - [httpcore.connection:87] - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f09e5ccf740>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - send_request_headers.started request=<Request [b'POST']>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - send_request_headers.complete
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - send_request_body.started request=<Request [b'POST']>
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - send_request_body.complete
2025-11-14 09:02:44 - DEBUG - [httpcore.http11:87] - receive_response_headers.started request=<Request [b'POST']>
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 14 Nov 2025 09:02:43 GMT'), (b'server', b'uvicorn'), (b'content-length', b'668'), (b'content-type', b'application/json')])
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - receive_response_body.started request=<Request [b'POST']>
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - receive_response_body.complete
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - response_closed.started
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - response_closed.complete
2025-11-14 09:03:16 - INFO - [services.llm_client:81] - ‚úì LLM call successful: url=http://192.168.51.22:8002, latency=31888.8ms, response_length=423
2025-11-14 09:03:16 - INFO - [api.routes:202] - [7d9a7e73] ‚úì Thinking complete (thoughts: 423 chars, enriched query: 451 chars)
2025-11-14 09:03:16 - DEBUG - [api.routes:203] - [7d9a7e73] Thoughts: 1. Understand the context: The user has not provided any specific details about what they are seeking assistance with.
2. Identify key concepts: The main concept here seems to be related to communicat...
2025-11-14 09:03:16 - DEBUG - [api.routes:206] - [7d9a7e73] STEP 3: Routing query
2025-11-14 09:03:16 - DEBUG - [services.llm_client:63] - Calling LLM: url=http://192.168.51.22:8002/v1/chat/completions, messages=2, max_tokens=10, temp=0.0
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - send_request_headers.started request=<Request [b'POST']>
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - send_request_headers.complete
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - send_request_body.started request=<Request [b'POST']>
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - send_request_body.complete
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - receive_response_headers.started request=<Request [b'POST']>
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 14 Nov 2025 09:02:43 GMT'), (b'server', b'uvicorn'), (b'content-length', b'249'), (b'content-type', b'application/json')])
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - receive_response_body.started request=<Request [b'POST']>
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - receive_response_body.complete
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - response_closed.started
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - response_closed.complete
2025-11-14 09:03:16 - INFO - [services.llm_client:81] - ‚úì LLM call successful: url=http://192.168.51.22:8002, latency=482.4ms, response_length=6
2025-11-14 09:03:16 - INFO - [api.routes:208] - [7d9a7e73] ‚úì Route decision: simple
2025-11-14 09:03:16 - DEBUG - [api.routes:211] - [7d9a7e73] STEP 4: Generating response
2025-11-14 09:03:16 - DEBUG - [api.routes:214] - [7d9a7e73] Calling Qwen at http://192.168.51.22:8002
2025-11-14 09:03:16 - DEBUG - [services.llm_client:63] - Calling LLM: url=http://192.168.51.22:8002/v1/chat/completions, messages=1, max_tokens=50, temp=0.7
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - send_request_headers.started request=<Request [b'POST']>
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - send_request_headers.complete
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - send_request_body.started request=<Request [b'POST']>
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - send_request_body.complete
2025-11-14 09:03:16 - DEBUG - [httpcore.http11:87] - receive_response_headers.started request=<Request [b'POST']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 14 Nov 2025 09:02:43 GMT'), (b'server', b'uvicorn'), (b'content-length', b'277'), (b'content-type', b'application/json')])
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - receive_response_body.started request=<Request [b'POST']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - receive_response_body.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - response_closed.started
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - response_closed.complete
2025-11-14 09:03:18 - INFO - [services.llm_client:81] - ‚úì LLM call successful: url=http://192.168.51.22:8002, latency=1918.0ms, response_length=34
2025-11-14 09:03:18 - INFO - [api.routes:223] - [7d9a7e73] ‚úì Qwen response received (length: 34 chars)
2025-11-14 09:03:18 - DEBUG - [api.routes:268] - [7d9a7e73] STEP 5: Storing memory
2025-11-14 09:03:18 - DEBUG - [httpcore.connection:87] - close.started
2025-11-14 09:03:18 - DEBUG - [httpcore.connection:87] - close.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.connection:87] - connect_tcp.started host='192.168.51.22' port=8001 local_address=None timeout=30.0 socket_options=None
2025-11-14 09:03:18 - DEBUG - [httpcore.connection:87] - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f09e5ccff80>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - send_request_headers.started request=<Request [b'POST']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - send_request_headers.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - send_request_body.started request=<Request [b'POST']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - send_request_body.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - receive_response_headers.started request=<Request [b'POST']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 14 Nov 2025 09:03:18 GMT'), (b'server', b'uvicorn'), (b'content-length', b'22005'), (b'content-type', b'application/json')])
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - receive_response_body.started request=<Request [b'POST']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - receive_response_body.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - response_closed.started
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:87] - response_closed.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.connection:47] - connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2025-11-14 09:03:18 - DEBUG - [httpcore.connection:47] - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f09e5cce510>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - send_request_headers.started request=<Request [b'PUT']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - send_request_headers.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - send_request_body.started request=<Request [b'PUT']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - send_request_body.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - receive_response_headers.started request=<Request [b'PUT']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'gzip'), (b'date', b'Fri, 14 Nov 2025 09:03:18 GMT')])
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - receive_response_body.started request=<Request [b'PUT']>
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - receive_response_body.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - response_closed.started
2025-11-14 09:03:18 - DEBUG - [httpcore.http11:47] - response_closed.complete
2025-11-14 09:03:18 - DEBUG - [httpcore.connection:47] - close.started
2025-11-14 09:03:18 - DEBUG - [httpcore.connection:47] - close.complete
2025-11-14 09:03:18 - INFO - [api.routes:284] - [7d9a7e73] ‚úì Memory stored for user=test-user
2025-11-14 09:03:18 - INFO - [api.routes:292] - [7d9a7e73] ‚úÖ Request complete: model=qwen-2.5-1.5b, rag=False, latency=35276.1ms
INFO:     127.0.0.1:43382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
